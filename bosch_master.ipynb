{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "bosch38",
      "language": "python",
      "name": "bosch38"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    },
    "colab": {
      "name": "bosch_master.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dustedduke/CNN-MFCC-sound-detection/blob/master/bosch_master.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6frT7pO5Q6h"
      },
      "source": [
        "# !pip install pytorch-lightning torchaudio optuna torchensemble neptune-client[optuna]\n",
        "# !git clone https://github.com/google-research/leaf-audio.git && cd leaf-audio && pip install -e ."
      ],
      "id": "n6frT7pO5Q6h",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vh8eTMTN5mEK"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")\n",
        "!cd /content/gdrive/MyDrive"
      ],
      "id": "Vh8eTMTN5mEK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Do29likB6Dek"
      },
      "source": [
        "# run = neptune.init(\n",
        "#     project=\"dustedduke/Bosch\",\n",
        "#     api_token=\"\",\n",
        "# )\n",
        "\n",
        "# params = {\"learning_rate\": 0.001, \"optimizer\": \"BCEWithLogitsLoss\"}\n",
        "# run[\"parameters\"] = params"
      ],
      "id": "Do29likB6Dek",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqhzdf4B59MS"
      },
      "source": [
        "dataset_folder = \"/content/gdrive/MyDrive/Datasets/UrbanSound8K\""
      ],
      "id": "hqhzdf4B59MS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6f07b8f5-5fa7-45ec-9d62-f64b5b19c5ca"
      },
      "source": [
        "import os, math, random, functools\n",
        "from time import time\n",
        "from pathlib import Path\n",
        "from argparse import ArgumentParser\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset, Subset\n",
        "from torch.optim import Adam\n",
        "\n",
        "from pytorch_lightning import Trainer, seed_everything, loggers\n",
        "from pytorch_lightning.core.lightning import LightningModule\n",
        "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "import torchaudio\n",
        "from torchaudio import transforms\n",
        "\n",
        "# import openl3\n",
        "# import soundfile as sf\n",
        "from leaf_audio import frontend, initializers\n",
        "\n",
        "import optuna\n",
        "import neptune.new as neptune\n",
        "import neptune.new.integrations.optuna as optuna_utils"
      ],
      "id": "6f07b8f5-5fa7-45ec-9d62-f64b5b19c5ca",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7657f04d-be65-4389-8d94-ae4259af58c4"
      },
      "source": [
        "## Audio transforms"
      ],
      "id": "7657f04d-be65-4389-8d94-ae4259af58c4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bbe3209-b8b5-4466-9d08-9a3121f46ecc"
      },
      "source": [
        "class AudioUtil():\n",
        "\n",
        "    @staticmethod\n",
        "    def get_available_transforms():\n",
        "      method_list = []\n",
        "      for attribute in dir(AudioUtil):\n",
        "          attribute_value = getattr(AudioUtil, attribute)\n",
        "          if callable(attribute_value):\n",
        "              if attribute.startswith('__') == False:\n",
        "                  method_list.append(attribute)\n",
        "      method_list.remove('open')\n",
        "      method_list.remove('get_available_transforms')\n",
        "      return method_list\n",
        "\n",
        "    # ----------------------------\n",
        "    # Load an audio file. Return the signal as a tensor and the sample rate\n",
        "    # ----------------------------\n",
        "    @staticmethod\n",
        "    def open(audio_file):\n",
        "        sig, sr = torchaudio.load(audio_file)\n",
        "        return (sig, sr)\n",
        "    \n",
        "    # ----------------------------\n",
        "    # Convert the given audio to the desired number of channels\n",
        "    # ----------------------------\n",
        "    @staticmethod\n",
        "    def rechannel(aud, new_channel):\n",
        "        sig, sr = aud\n",
        "\n",
        "        if (sig.shape[0] == new_channel):\n",
        "            # Nothing to do\n",
        "            return aud\n",
        "\n",
        "        if (new_channel == 1):\n",
        "            # Convert from stereo to mono by selecting only the first channel\n",
        "            resig = sig[:1, :]\n",
        "        else:\n",
        "            # Convert from mono to stereo by duplicating the first channel\n",
        "            resig = torch.cat([sig, sig])\n",
        "\n",
        "        return ((resig, sr))\n",
        "    \n",
        "    # ----------------------------\n",
        "    # Since Resample applies to a single channel, we resample one channel at a time\n",
        "    # ----------------------------\n",
        "    @staticmethod\n",
        "    def resample(aud, newsr):\n",
        "        sig, sr = aud\n",
        "\n",
        "        if (sr == newsr):\n",
        "            # Nothing to do\n",
        "            return aud\n",
        "\n",
        "        num_channels = sig.shape[0]\n",
        "        # Resample first channel\n",
        "        resig = torchaudio.transforms.Resample(sr, newsr)(sig[:1,:])\n",
        "        if (num_channels > 1):\n",
        "            # Resample the second channel and merge both channels\n",
        "            retwo = torchaudio.transforms.Resample(sr, newsr)(sig[1:,:])\n",
        "            resig = torch.cat([resig, retwo])\n",
        "\n",
        "        return ((resig, newsr))\n",
        "    \n",
        "    # ----------------------------\n",
        "    # Pad (or truncate) the signal to a fixed length 'max_ms' in milliseconds\n",
        "    # ----------------------------\n",
        "    @staticmethod\n",
        "    def pad_trunc(aud, max_ms):\n",
        "        sig, sr = aud\n",
        "        num_rows, sig_len = sig.shape\n",
        "        max_len = sr//1000 * max_ms\n",
        "\n",
        "        if (sig_len > max_len):\n",
        "            # Truncate the signal to the given length\n",
        "            sig = sig[:,:max_len]\n",
        "\n",
        "        elif (sig_len < max_len):\n",
        "            # Length of padding to add at the beginning and end of the signal\n",
        "            pad_begin_len = random.randint(0, max_len - sig_len)\n",
        "            pad_end_len = max_len - sig_len - pad_begin_len\n",
        "\n",
        "            # Pad with 0s\n",
        "            pad_begin = torch.zeros((num_rows, pad_begin_len))\n",
        "            pad_end = torch.zeros((num_rows, pad_end_len))\n",
        "\n",
        "            sig = torch.cat((pad_begin, sig, pad_end), 1)\n",
        "\n",
        "        sig = torch.t(sig)\n",
        "        return sig\n",
        "    \n",
        "    # ----------------------------\n",
        "    # Shifts the signal to the left or right by some percent. Values at the end\n",
        "    # are 'wrapped around' to the start of the transformed signal.\n",
        "    # ----------------------------\n",
        "    @staticmethod\n",
        "    def time_shift(aud, shift_limit):\n",
        "        sig,sr = aud\n",
        "        _, sig_len = sig.shape\n",
        "        shift_amt = int(random.random() * shift_limit * sig_len)\n",
        "        return (sig.roll(shift_amt), sr)\n",
        "    \n",
        "    # ----------------------------\n",
        "    # Generate a Spectrogram\n",
        "    # ----------------------------\n",
        "    @staticmethod\n",
        "    def spectro_gram(aud, n_mels=64, n_fft=1024, hop_len=None):\n",
        "        sig,sr = aud\n",
        "        top_db = 80\n",
        "\n",
        "        # spec has shape [channel, n_mels, time], where channel is mono, stereo etc\n",
        "        spec = transforms.MelSpectrogram(sr, n_fft=n_fft, hop_length=hop_len, n_mels=n_mels)(sig)\n",
        "\n",
        "        # Convert to decibels\n",
        "        spec = transforms.AmplitudeToDB(top_db=top_db)(spec)\n",
        "        return (spec)\n",
        "    \n",
        "    # ----------------------------\n",
        "    # Augment the Spectrogram by masking out some sections of it in both the frequency\n",
        "    # dimension (ie. horizontal bars) and the time dimension (vertical bars) to prevent\n",
        "    # overfitting and to help the model generalise better. The masked sections are\n",
        "    # replaced with the mean value.\n",
        "    # ----------------------------\n",
        "    @staticmethod\n",
        "    def spectro_augment(spec, max_mask_pct=0.1, n_freq_masks=1, n_time_masks=1):\n",
        "        _, n_mels, n_steps = spec.shape\n",
        "        mask_value = spec.mean()\n",
        "        aug_spec = spec\n",
        "\n",
        "        freq_mask_param = max_mask_pct * n_mels\n",
        "        for _ in range(n_freq_masks):\n",
        "            aug_spec = transforms.FrequencyMasking(freq_mask_param)(aug_spec, mask_value)\n",
        "\n",
        "        time_mask_param = max_mask_pct * n_steps\n",
        "        for _ in range(n_time_masks):\n",
        "            aug_spec = transforms.TimeMasking(time_mask_param)(aug_spec, mask_value)\n",
        "\n",
        "        return aug_spec\n",
        "\n",
        "    def original_augmentation(aud, sr):\n",
        "        reaud = resample(aud, sr)\n",
        "        rechan = rechannel(reaud, self.channel)\n",
        "        dur_aud = pad_trunc(rechan, self.duration)\n",
        "        shift_aud = time_shift(dur_aud, self.shift_pct)\n",
        "        sgram = spectro_gram(shift_aud, n_mels=64, n_fft=1024, hop_len=None)\n",
        "        aug_sgram = spectro_augment(sgram, max_mask_pct=0.1, n_freq_masks=2, n_time_masks=2)\n",
        "\n",
        "        return aug_sgram\n",
        "\n",
        "    # TODO fix\n",
        "    def rolling_spectro_augmentation(aud, sr, step):\n",
        "        n_samples = 2 * int(aud.shape[1]/sr * 0.1)\n",
        "        complete_sample = torch.empty((0,0))\n",
        "\n",
        "        _min, _max = float('inf'), -float('inf')\n",
        "        for idx in range(n_samples):\n",
        "          rand_index = np.random.randint(0, aud.shape[1] - step)\n",
        "          sliced_aud = aud[:, rand_index:rand_index+step]\n",
        "          sample_spectro = spectro_gram(sliced_aud, n_mels=64, n_fft=1024, hop_len=None)\n",
        "          _min = min(np.amin(sample_spectro), _min)\n",
        "          _max = min(np.amax(sample_spectro), _max)\n",
        "          complete_sample = torch.cat((complete_sample, sample_spectro), 1)\n",
        "          complete_sample = (complete_sample - _min) / (_max - _min)\n",
        "        \n",
        "        return complete_sample\n",
        "    \n",
        "    def leaf_embedding(aud, n_filters=64, window_len=32, sample_rate=24000, preemp=True):\n",
        "        compression_fn = functools.partial(frontend.log_compression, log_offset=1e-5)\n",
        "        complex_conv_init = initializers.GaborInit(sample_rate=sample_rate, min_freq=60., max_freq=7800.)\n",
        "        learn_pooling=False\n",
        "        custom_leaf = frontend.Leaf(learn_pooling=learn_pooling,\n",
        "                                    n_filters=n_filters,\n",
        "                                    window_len=window_len,\n",
        "                                    sample_rate=sample_rate,\n",
        "                                    preemp=preemp,\n",
        "                                    compression_fn=compression_fn,\n",
        "                                    complex_conv_init=complex_conv_init)\n",
        "\n",
        "        sig, sr = aud\n",
        "        np_tensor = sig.numpy()[0].reshape(1,sig[0].Size())\n",
        "        aud_tf = tf.convert_to_tensor(np_tensor) \n",
        "        basic_leaf = torch.from_numpy(frontend.Leaf()[0].numpy())\n",
        "\n",
        "        return basic_leaf\n",
        "    \n",
        "    def gru_embedding():\n",
        "        # TODO in progress\n",
        "        ...\n",
        "\n"
      ],
      "id": "1bbe3209-b8b5-4466-9d08-9a3121f46ecc",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62cb9651-e672-4fc6-bb9d-0b75eaeeefaa"
      },
      "source": [
        "## Dataset"
      ],
      "id": "62cb9651-e672-4fc6-bb9d-0b75eaeeefaa"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3bcf699-7ca5-48dc-b427-3dceb4a46a00"
      },
      "source": [
        "def one_hot(idx, num_items):\n",
        "    return [(0.0 if n != idx else 1.0) for n in range(num_items)]\n",
        "\n",
        "class UrbanDataset(Dataset):\n",
        "\n",
        "    def __init__(self, dataset_folder, fold, transform=None, augment=None):\n",
        "        super().__init__()\n",
        "        self.dataset_folder = dataset_folder\n",
        "        self.path_to_csv = os.path.join(self.dataset_folder, \"metadata/UrbanSound8K.csv\")\n",
        "        self.path_to_audio_folder = os.path.join(self.dataset_folder, \"audio\")\n",
        "        self.metadata = pd.read_csv(self.path_to_csv)\n",
        "        self.fold = fold\n",
        "        self.transform = transform\n",
        "        self.augment = augment\n",
        "        \n",
        "    def train_validation_split(self):\n",
        "        train_idx = list(self.metadata[self.metadata[\"fold\"] != self.fold].index)\n",
        "        val_idx = list(self.metadata[self.metadata[\"fold\"] == self.fold].index)\n",
        "\n",
        "        train_set = Subset(self, train_idx)\n",
        "        val_set = Subset(self, val_idx)\n",
        "        val_set.augmenation = None\n",
        "\n",
        "        return train_set, val_set\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.metadata)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        file_name = self.metadata[\"slice_file_name\"].iloc[index]\n",
        "        file_path = os.path.join(\n",
        "            os.path.join(self.path_to_audio_folder, \"fold\" + str(self.metadata[\"fold\"].iloc[index])), file_name\n",
        "        )\n",
        "\n",
        "        aud, sr = torchaudio.load(file_path)\n",
        "        \n",
        "        # Both training and validation are transformed\n",
        "        aud = self.transform(aud)\n",
        "\n",
        "        # Augment if not from validation fold\n",
        "        if self.augment and self.metadata[\"fold\"].iloc[index] != self.fold:\n",
        "            aud = self.augment(aud)\n",
        "\n",
        "        label = np.array(one_hot(self.metadata[\"classID\"].iloc[index], 10))\n",
        "\n",
        "        return {\n",
        "            \"file_name\": file_name,\n",
        "            \"input_vector\": aud,\n",
        "            \"label\": label,\n",
        "        }\n"
      ],
      "id": "d3bcf699-7ca5-48dc-b427-3dceb4a46a00",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75b99774-f58f-4752-8811-4b00bea44fb4"
      },
      "source": [
        "## Base Model"
      ],
      "id": "75b99774-f58f-4752-8811-4b00bea44fb4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7d20eef9-3ef3-475b-905c-7350125ce6ad"
      },
      "source": [
        "class VanillaModel(nn.Module):\n",
        "    \n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_layers, output_dim, drop_prob):\n",
        "        super().__init__()\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.drop_prob = drop_prob\n",
        "\n",
        "        self.previous_hidden = None\n",
        "\n",
        "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=self.embedding_dim, \n",
        "            hidden_size=self.hidden_dim, \n",
        "            num_layers=self.n_layers, \n",
        "            batch_first=True, \n",
        "            dropout=self.drop_prob,\n",
        "            bidirectional=False)\n",
        "        self.fc = nn.Linear(self.hidden_dim, self.output_dim)\n",
        "        \n",
        "    def forward(self, x, hidden):\n",
        "        batch_size = x.size(0)\n",
        "        x = x.long()\n",
        "        embeds = self.embedding(x)\n",
        "        x, hidden = self.lstm(embeds, self.previous_hidden)\n",
        "        self.previous_hidden = detach_tensors(hidden) \n",
        "        x = x[:, -1]\n",
        "        x = self.fc(x)\n",
        "\n",
        "    def init_hidden():\n",
        "        # self.previous_hidden = (torch.zeros(1, 10, 2), torch.zeros(1, 10, 2))\n",
        "        nn.init.xavier_uniform_(previous_hidden[0])\n",
        "        nn.init.xavier_uniform_(previous_hidden[1])\n",
        "\n",
        "        return x"
      ],
      "id": "7d20eef9-3ef3-475b-905c-7350125ce6ad",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da2e04cd-f805-4703-b012-6b116c6113dd"
      },
      "source": [
        "## Lightning Wrapper"
      ],
      "id": "da2e04cd-f805-4703-b012-6b116c6113dd"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2fc05ce-2679-4923-8cd1-e31d91f64cc9"
      },
      "source": [
        "class VanillaClassifier(LightningModule):\n",
        "    def __init__(self, hparams, fold):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hparams = hparams\n",
        "        self.fold = fold\n",
        "\n",
        "        self.dataset_folder = hparams.path_to_UrbanSound8K\n",
        "        self.nb_classes = hparams.output_dim\n",
        "        self.best_scores = [0] * 5\n",
        "        \n",
        "        self.vocab_size = hparams.vocab_size\n",
        "        self.output_size = hparams.output_size\n",
        "        self.embedding_dim = hparams.embedding_dim\n",
        "        self.hidden_dim = hparams.hidden_dim\n",
        "        self.n_layers = hparams.n_layers\n",
        "        self.drop_prob = hparams.drop_prob\n",
        "        \n",
        "        model_param = {\"classes_num\": self.nb_classes, \n",
        "                       \"vocab_size\": self.vocab_size,\n",
        "                       \"output_size\": self.output_size,\n",
        "                       \"embedding_dim\": self.embedding_dim,\n",
        "                       \"hidden_dim\": self.hidden_dim,\n",
        "                       \"n_layers\": self.n_layers,\n",
        "                       \"drop_prob\": self.drop_prob}\n",
        "\n",
        "        self.model = VanillaModel(**model_param)\n",
        "        self.loss = hparams.loss\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        return x\n",
        "\n",
        "    def prepare_data(self):\n",
        "          data_param = {\n",
        "              \"dataset_folder\": self.dataset_folder,\n",
        "              \"fold\": self.fold,\n",
        "              \"transform\": self.hparams.transform,\n",
        "              \"augment\": self.hparams.augment\n",
        "          }\n",
        "          self.dataset = UrbanDataset(**data_param)\n",
        "          (self.train_dataset, self.val_dataset) = self.dataset.train_validation_split()\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.train_dataset, batch_size=self.hparams.batch_size, shuffle=self.hparams['shuffle'], num_workers=8, drop_last=True\n",
        "        )\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.val_dataset, batch_size=self.hparams['batch_size'], num_workers=8, drop_last=True)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.model.parameters(), lr=self.hparams['learning_rate'])\n",
        "        \n",
        "    def training_step(self, batch, batch_idx):\n",
        "        data, target_c = (\n",
        "            batch[\"input_vector\"].float(),\n",
        "            batch[\"label\"].double()\n",
        "        )\n",
        "        target = torch.cat([target_c], 1)\n",
        "        output = self.forward(data)\n",
        "        loss = self.loss(output, target_c).mean(0)\n",
        "\n",
        "        return {\"loss\": loss, \"log\": {\"1_loss/train_loss\": loss}}\n",
        "\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        data, target_c= (\n",
        "            batch[\"input_vector\"].float(),\n",
        "            batch[\"label\"].double()\n",
        "        )\n",
        "\n",
        "        target = torch.cat([target_c], 1)\n",
        "        output = self.forward(data)\n",
        "        loss = torch.cat([self.loss(output, target_c).mean(0), self.loss_c(output, target_c).mean(0)], 0)\n",
        "\n",
        "        return {\n",
        "            \"val_loss\": loss,\n",
        "            \"output\": outputs_c,\n",
        "            \"target\": target_c,\n",
        "        }\n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        val_loss = torch.cat([o[\"val_loss\"] for o in outputs], 0).mean()\n",
        "        all_outputs = torch.cat([o[\"output\"] for o in outputs], 0).cpu().numpy()\n",
        "        all_targets = torch.cat([o[\"target\"] for o in outputs], 0).cpu().numpy()\n",
        "\n",
        "        accuracy_score = accuracy(all_targets, all_outputs)\n",
        "        f1_micro = compute_micro_F1(all_targets, all_outputs)\n",
        "        auprc_micro = compute_micro_auprc(all_targets, all_outputs)\n",
        "        _, auprc_macro = compute_macro_auprc(all_targets, all_outputs, True)\n",
        "        map_score = mean_average_precision(all_targets, all_outputs)\n",
        "\n",
        "        if accuracy_score > self.best_scores[0]:\n",
        "            self.best_scores[0] = accuracy_score\n",
        "        if f1_micro > self.best_scores[1]:\n",
        "            self.best_scores[1] = f1_micro\n",
        "        if auprc_micro > self.best_scores[2]:\n",
        "            self.best_scores[2] = auprc_micro\n",
        "        if auprc_macro > self.best_scores[3]:\n",
        "            self.best_scores[3] = auprc_macro\n",
        "        if map_score > self.best_scores[4]:\n",
        "            self.best_scores[4] = map_score\n",
        "\n",
        "        log_temp = {\n",
        "            \"2_valid/1_accuracy0.5\": accuracy_score,\n",
        "            \"2_valid/1_f1_micro0.5\": f1_micro,\n",
        "            \"2_valid/1_auprc_micro\": auprc_micro,\n",
        "            \"2_valid/1_auprc_macro\": auprc_macro,\n",
        "            \"2_valid/1_map\": map_score,\n",
        "        }\n",
        "\n",
        "        tqdm_dict = {\n",
        "            \"val_loss\": val_loss,\n",
        "            \"acc\": accuracy_score,\n",
        "        }\n",
        "\n",
        "        log = {\n",
        "            \"step\": self.current_epoch,\n",
        "            \"1_loss/val_loss\": val_loss,\n",
        "        }\n",
        "\n",
        "        log.update(log_temp)\n",
        "\n",
        "        return {\"progress_bar\": tqdm_dict, \"log\": log}\n",
        "\n",
        "    def test_step():\n",
        "        ..."
      ],
      "id": "c2fc05ce-2679-4923-8cd1-e31d91f64cc9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0117c36-4fe3-4da2-a460-74c42b7787d3"
      },
      "source": [
        "## Training"
      ],
      "id": "f0117c36-4fe3-4da2-a460-74c42b7787d3"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24d6b25b-da26-431a-8d83-dd67a9b9460f"
      },
      "source": [
        "def training(hparams, fold):\n",
        "    seed_everything(hparams.seed)\n",
        "    MAIN_DIR = os.path.join(config.path_to_summaries, \"LSTMUrbanSounds/\")\n",
        "    \n",
        "    model = VanillaClassifier(hparams, fold)\n",
        "    \n",
        "    tb_logger = loggers.TensorBoardLogger(os.path.join(MAIN_DIR, \"logs\"))\n",
        "    early_stopping = EarlyStopping(\"2_valid/1_accuracy0.5\", patience=50, mode=\"max\")\n",
        "    \n",
        "    trainer = Trainer.from_argparse_args(\n",
        "        hparams,\n",
        "        default_root_dir=MAIN_DIR,\n",
        "        logger=tb_logger,\n",
        "#         callbacks=[early_stopping],\n",
        "        gpus=0,\n",
        "    )\n",
        "    \n",
        "    trainer.fit(model)\n",
        "    return model.best_scores\n",
        "\n",
        "    with open(os.path.join(MAIN_DIR, \"logs/report.txt\"), \"a\") as file:\n",
        "        file.write(hparams.dataset + \"\\n\")\n",
        "        file.write(str(model.best_scores) + \"\\n\")"
      ],
      "id": "24d6b25b-da26-431a-8d83-dd67a9b9460f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27b246bb-be76-419f-b458-eaa6d9a74289"
      },
      "source": [
        "## Training with folds"
      ],
      "id": "27b246bb-be76-419f-b458-eaa6d9a74289"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10ded66e-f6c0-454a-8b08-e4e29c84f5b7"
      },
      "source": [
        "def train_with_folds(hparams, folds):\n",
        "    for i in range(1, folds + 1):\n",
        "        metrics = training(hparams, i)"
      ],
      "id": "10ded66e-f6c0-454a-8b08-e4e29c84f5b7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f85e986c-5bd8-4fb1-8f91-ff2a766c5f73"
      },
      "source": [
        "# Configure and Start"
      ],
      "id": "f85e986c-5bd8-4fb1-8f91-ff2a766c5f73"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnRiiGS7vh0M"
      },
      "source": [
        "## 1. Choose list of transforms and augmentations"
      ],
      "id": "mnRiiGS7vh0M"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYsVE9YsvgZ3",
        "outputId": "b619c420-986f-4fd4-fc55-51213f562268"
      },
      "source": [
        "print('AVAILABLE TRANSFORMS AND AUGMENTATIONS:', *AudioUtil.get_available_transforms(), sep='\\n- ')"
      ],
      "id": "BYsVE9YsvgZ3",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AVAILABLE TRANSFORMS AND AUGMENTATIONS:\n",
            "- gru_embedding\n",
            "- leaf_embedding\n",
            "- original_augmentation\n",
            "- pad_trunc\n",
            "- rechannel\n",
            "- resample\n",
            "- rolling_spectro_augmentation\n",
            "- spectro_augment\n",
            "- spectro_gram\n",
            "- time_shift\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viFPFOA23dkV"
      },
      "source": [
        "neptune_callback = optuna_utils.NeptuneCallback(run)\n",
        "\n",
        "CONFIGURATION = {\n",
        "    'transforms': ['leaf_embedding']\n",
        "}"
      ],
      "id": "viFPFOA23dkV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ed09b315-92c8-45f8-8cd3-802f604cef92"
      },
      "source": [
        "def objective(trial):\n",
        "\n",
        "    folds = trial.suggest_int(\"folds\", 1, 1) #11\n",
        "    loss = trial.suggest_categorical(\"loss\", [BCEWithLogitsLoss(reduction=\"none\")])\n",
        "    shuffle = False\n",
        "\n",
        "    # LSTM parameters \n",
        "    embedding_dim = trial.suggest_int(\"embedding_dim\", 2, 4)\n",
        "    hidden_dim = trial.suggest_int(\"hidden_dim\", 4, 6)\n",
        "    n_layers = trial.suggest_int(\"hidden_dim\", 1, 2)\n",
        "    drop_prob = trial.suggest_float(\"drop_prob\", 0.2, 0.5)\n",
        "    batch_size = trial.suggest_int(\"batch_size\", 32, 256, log=True)\n",
        "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-2, log=True)\n",
        "\n",
        "    # LEAF parameters\n",
        "    sample_rate = trial.suggest_categorical(\"sample_rate\", [24000, 44100, 48000])\n",
        "    preemp = trial.suggest_categorical(\"preemp\", [True, False])\n",
        "\n",
        "    # Rechannel parameters\n",
        "    channels = trial.suggest_int(\"hidden_dim\", 1, 1)\n",
        "\n",
        "    # Time Shift parameters\n",
        "    shift_limit = trial.suggest_int(\"shift_limit\", 10, 50, step=10)\n",
        "\n",
        "    # Spectrogram parameters\n",
        "    n_mels = trial.suggest_int(\"n_mels\", 4, 4, log=True)\n",
        "    n_fft = trial.suggest_int(\"n_fft\", 8, 8, log=True)\n",
        "    hop_len = trial.suggest_int(\"hop_len\", 256, 512, log=True)\n",
        "\n",
        "    # SpectroFreqTimeAugment parameters\n",
        "    max_mask_pct = trial.suggest_int(\"max_mask_pct\", 1, 2)\n",
        "    n_freq_masks = trial.suggest_int(\"n_freq_masks\", 1, 2)\n",
        "    n_time_masks = trial.suggest_float(\"n_time_masks\", 0.1, 1.2)\n",
        "\n",
        "\n",
        "    CONFIGURATION['folds'] = folds\n",
        "    CONFIGURATION['embedding_dim'] = embedding_dim\n",
        "    CONFIGURATION['hidden_dim'] = hidden_dim\n",
        "    CONFIGURATION['output_dim'] = 10\n",
        "    CONFIGURATION['n_layers'] = n_layers\n",
        "    CONFIGURATION['drop_prob'] = drop_prob\n",
        "    CONFIGURATION['batch_size'] = batch_size\n",
        "    CONFIGURATION['learning_rate'] = learning_rate\n",
        "    CONFIGURATION['loss'] = loss\n",
        "    CONFIGURATION['shuffle'] = shuffle\n",
        "\n",
        "    CONFIGURATION['sample_rate'] = sample_rate\n",
        "    CONFIGURATION['preemp'] = preemp\n",
        "\n",
        "    CONFIGURATION['shift_limit'] = shift_limit\n",
        "\n",
        "    CONFIGURATION['n_mels'] = n_mels\n",
        "    CONFIGURATION['n_fft'] = n_fft\n",
        "    CONFIGURATION['hop_len'] = hop_len\n",
        "\n",
        "    CONFIGURATION['max_mask_pct'] = max_mask_pct\n",
        "    CONFIGURATION['n_freq_masks'] = n_freq_masks\n",
        "    CONFIGURATION['n_time_masks'] = n_time_masks\n",
        "\n",
        "    metrics = train_with_folds(CONFIGURATION, folds)\n",
        "\n",
        "    return metrics\n"
      ],
      "id": "ed09b315-92c8-45f8-8cd3-802f604cef92",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XsZRove3o2J"
      },
      "source": [
        "## 2. Run study"
      ],
      "id": "2XsZRove3o2J"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8fJJyk23Wum"
      },
      "source": [
        "study = optuna.create_study()\n",
        "study.optimize(objective, n_trials=100, callbacks=[neptune_callback])\n",
        "\n",
        "print(\"Number of finished trials: \", len(study.trials))\n",
        "\n",
        "trials = sorted(study.best_trials, key=lambda t: t.values)\n",
        "\n",
        "for trial in trials:\n",
        "    print(\"  Trial#{}\".format(trial.number))\n",
        "    print(\"    Values: FLOPS={}, accuracy={}\".format(trial.values[0], trial.values[1]))\n",
        "    print(\"    Params: {}\".format(trial.params))"
      ],
      "id": "D8fJJyk23Wum",
      "execution_count": null,
      "outputs": []
    }
  ]
}